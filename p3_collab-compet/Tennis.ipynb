{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06:57:05\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import Agent\n",
    "importlib.reload(Agent)\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "t = time.localtime()\n",
    "current_time = time.strftime(\"%H:%M:%S\", t)\n",
    "print(current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.5 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is already saved in the Workspace and can be accessed at the file path provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"/data/Tennis_Linux_NoVis/Tennis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "brain_name\n",
    "USE_MULTI_AGENT_ENV = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "states.shape (2, 24)\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.65278625 -1.5        -0.          0.\n",
      "  6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "# print('action_size:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "print('states.shape', states.shape)\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_states = env_info.vector_observations \n",
    "# test_states.__class__\n",
    "\n",
    "# next_states=[]\n",
    "# next_states.append(test_states[0])\n",
    "# np.asarray(all_agents_actions_next) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = None\n",
    "# b = None\n",
    "# for i in range(5):                                         # play game for 5 episodes\n",
    "#     env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "#     states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "#     scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "#     t = 0\n",
    "#     while True:\n",
    "#         t += 1\n",
    "#         actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "# #         print('before actions', actions)\n",
    "# #         actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "# #         print('after actions', actions)\n",
    "#         env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "#         next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "#         rewards = env_info.rewards                         # get reward (for each agent)\n",
    "#         dones = env_info.local_done                        # see if episode finished\n",
    "#         scores += env_info.rewards                         # update the score (for each agent)\n",
    "#         states = next_states                               # roll over states to next time step\n",
    "# #         print(i,'rewards:', rewards)\n",
    "        \n",
    "#         if np.any(dones):                                  # exit loop if episode finished\n",
    "#             if a is None:\n",
    "#                 a = actions\n",
    "#             elif b is None:\n",
    "#                 b = actions\n",
    "# #             torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "#             break\n",
    "#     print(i,'------episode done------')\n",
    "    \n",
    "#     print('Total score (averaged over agents) this episode: {} / max timestep {} '.format(np.mean(scores), t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready to train\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple, deque\n",
    "\n",
    "def get_path(suffix):\n",
    "    CHECKPOINT_PATH_ACTOR_BASE = 'checkpoint_actor{}_{}.pth' \n",
    "    CHECKPOINT_PATH_CRITIC_BASE = 'checkpoint_critic{}_{}.pth'\n",
    "    actors_path = [CHECKPOINT_PATH_ACTOR_BASE.format(1, suffix),CHECKPOINT_PATH_ACTOR_BASE.format(2, suffix)]\n",
    "    critics_path = [ CHECKPOINT_PATH_CRITIC_BASE.format(1, suffix),CHECKPOINT_PATH_CRITIC_BASE.format(2, suffix)]\n",
    "    return (actors_path, critics_path)\n",
    "\n",
    "def load_checkpoint(agent, path_suffix):\n",
    "    actor1_path_to_load, actor2_path_to_load, critic_path_to_load = get_path(path_suffix)\n",
    "    agent.actors_target[0].load_state_dict(torch.load(actor1_path_to_load))\n",
    "    agent.actors_target[1].load_state_dict(torch.load(actor2_path_to_load))\n",
    "    agent.actors_local[0].load_state_dict(torch.load(actor1_path_to_load))\n",
    "    agent.actors_local[1].load_state_dict(torch.load(actor2_path_to_load))\n",
    "    agent.critic_local.load_state_dict(torch.load(critic_path_to_load))\n",
    "    agent.critic_target.load_state_dict(torch.load(critic_path_to_load))\n",
    "    print('*****realoaded saved checkpoint******', actor1_path_to_load, critic_path_to_load)\n",
    "   \n",
    "print('ready to train')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running maddpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilities\n",
    "importlib.reload(utilities)\n",
    "\n",
    "LAST_BEST_SCORE=float('-inf')\n",
    "\n",
    "def ddpg_ma_version(agent, n_episodes, path_to_write_suffix,apply_noise=True, eval_mode=True, start_noise=10, noise_reduction=0.9999,min_noise=0.1, max_t=1000, last_best_score=LAST_BEST_SCORE, final_scores=[]):\n",
    "    actors_path,critics_path = get_path(path_to_write_suffix)\n",
    "    noise = start_noise if apply_noise else 0\n",
    "    noise_reduction = noise_reduction\n",
    "    print('--------start learning----------')\n",
    "    last_100_scores_deque = deque(maxlen=100)\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "#         states = env.reset()[brain_name].vector_observations\n",
    "        states = env.reset(train_mode=True)[brain_name].vector_observations\n",
    "        agent.reset()\n",
    "        ending_t = 0\n",
    "        all_agent_scores = []\n",
    "        reward_this_episode = np.zeros((1, 2))\n",
    "        update_iteration = 0\n",
    "        for t in range(max_t):\n",
    "            # make add_noise=True when early in the training stage;\n",
    "            # I changed it False at the end to verify the agent solves the environemnt\n",
    "            actions_for_2 = agent.act(states, noise=noise)\n",
    "            noise = max(noise*noise_reduction, min_noise)\n",
    "            actions_array = torch.stack(actions_for_2).detach().cpu().numpy()\n",
    "            actions_for_env = np.rollaxis(actions_array,1)\n",
    "            env_info= env.step(actions_for_env)[brain_name]\n",
    "            next_states_for_2 = env_info.vector_observations   # get the next state\n",
    "            rewards_for_2 = env_info.rewards                   # get the reward\n",
    "            done_for_2 = env_info.local_done\n",
    "            # if model has not been improving, then update the networks 10 times after every 20 timesteps\n",
    "            agent.add_to_memory(states, actions_array, rewards_for_2, next_states_for_2, done_for_2, i_episode)\n",
    "            if not eval_mode:\n",
    "                update_iteration = agent.learn_and_update(i_episode, t, start_update_episode=0, ts_per_update=1 , updates_per_ts=0.2, update_iteration=update_iteration)\n",
    "\n",
    "            reward_this_episode += rewards_for_2\n",
    "            states = next_states_for_2\n",
    "            all_agent_scores.append(rewards_for_2)\n",
    "            if np.any(done_for_2):\n",
    "                ending_t=t\n",
    "                break\n",
    "        c = np.vstack(all_agent_scores)\n",
    "        sum_all_agent_scores = c.sum(axis=0)\n",
    "        episode_max_score =  np.max(sum_all_agent_scores)\n",
    "        last_100_scores_deque.append(episode_max_score)\n",
    "        last_100_avg_score =  np.mean(last_100_scores_deque)\n",
    "        final_scores.append(episode_max_score)\n",
    "        if last_best_score < episode_max_score:\n",
    "            print('[...Saving model - episode {};T {} ; episode_max={:.2f}; last_100_avg_score={:.4f} path {}'.format(i_episode,ending_t, episode_max_score, last_100_avg_score,path_to_write_suffix))\n",
    "            last_best_score = episode_max_score\n",
    "            for agent_i in range(2):\n",
    "                torch.save(agent.maddpg_agent[agent_i].actor_local.state_dict(), actors_path[agent_i])\n",
    "                torch.save(agent.maddpg_agent[agent_i].critic_local.state_dict(), critics_path[agent_i])\n",
    "\n",
    "        if last_100_avg_score >= 0.5:\n",
    "            print('\\rSolved environment after {} Episodes\\t Last Max Agent Score: {:.2f}'.format(i_episode, episode_max_score))\n",
    "            return final_scores\n",
    "        \n",
    "        if last_100_avg_score <0.05 and i_episode >1500:\n",
    "            print('Early return:i_episode={}; last_100_avg_score={}'.format(i_episode, last_100_avg_score))\n",
    "            return\n",
    "\n",
    "\n",
    "        if i_episode%100 == 0:\n",
    "            print('\\rEpisode {}; Ending_T {} ; episode_max: {:.12f} ; last_100_avg_score {:.4f}; Buffer {}.'.format(i_episode, ending_t, episode_max_score,last_100_avg_score, agent.memory.__len__()))\n",
    "    print('----- Finish Training for {} Episodes, best score: {} | -------'.format(n_episodes, last_best_score))\n",
    "    return final_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_2 Critic\n",
      "model_2 Critic\n",
      "DDPGAgent: lr_actor=0.0002 ; lr_critic=0.001\n",
      "DDPGAgent: lr_actor=0.0002 ; lr_critic=0.001\n",
      "ReplayBuffer-device: cuda\n",
      "--------start learning----------\n",
      "[...Saving model - episode 1;T 13 ; episode_max=0.00; last_100_avg_score=0.0000 path 6_11_train_v2\n",
      "[...Saving model - episode 4;T 30 ; episode_max=0.09; last_100_avg_score=0.0225 path 6_11_train_v2\n",
      "[...Saving model - episode 7;T 51 ; episode_max=0.10; last_100_avg_score=0.0271 path 6_11_train_v2\n",
      "[...Saving model - episode 69;T 67 ; episode_max=0.19; last_100_avg_score=0.0126 path 6_11_train_v2\n",
      "Episode 100; Ending_T 14 ; episode_max: 0.000000000000 ; last_100_avg_score 0.0097; Buffer 1688.\n",
      "Episode 200; Ending_T 13 ; episode_max: 0.000000000000 ; last_100_avg_score 0.0057; Buffer 3257.\n",
      "Episode 300; Ending_T 13 ; episode_max: 0.000000000000 ; last_100_avg_score 0.0147; Buffer 5088.\n",
      "Episode 400; Ending_T 13 ; episode_max: 0.000000000000 ; last_100_avg_score 0.0079; Buffer 6689.\n",
      "Episode 500; Ending_T 13 ; episode_max: 0.000000000000 ; last_100_avg_score 0.0158; Buffer 8528.\n",
      "Episode 600; Ending_T 28 ; episode_max: 0.100000001490 ; last_100_avg_score 0.0118; Buffer 10197.\n",
      "[...Saving model - episode 641;T 70 ; episode_max=0.20; last_100_avg_score=0.0128 path 6_11_train_v2\n",
      "Episode 700; Ending_T 59 ; episode_max: 0.100000001490 ; last_100_avg_score 0.0198; Buffer 12103.\n",
      "Episode 800; Ending_T 33 ; episode_max: 0.100000001490 ; last_100_avg_score 0.0221; Buffer 14000.\n",
      "Episode 900; Ending_T 13 ; episode_max: 0.000000000000 ; last_100_avg_score 0.0289; Buffer 16205.\n",
      "Episode 1000; Ending_T 13 ; episode_max: 0.000000000000 ; last_100_avg_score 0.0422; Buffer 18852.\n",
      "Episode 1100; Ending_T 48 ; episode_max: 0.100000001490 ; last_100_avg_score 0.0462; Buffer 21583.\n",
      "Episode 1200; Ending_T 13 ; episode_max: 0.000000000000 ; last_100_avg_score 0.0459; Buffer 24382.\n",
      "Episode 1300; Ending_T 29 ; episode_max: 0.100000001490 ; last_100_avg_score 0.0422; Buffer 26913.\n",
      "Episode 1400; Ending_T 31 ; episode_max: 0.090000001714 ; last_100_avg_score 0.0428; Buffer 29148.\n",
      "[...Saving model - episode 1445;T 537 ; episode_max=1.40; last_100_avg_score=0.0622 path 6_11_train_v2\n",
      "Episode 1500; Ending_T 29 ; episode_max: 0.090000001714 ; last_100_avg_score 0.0915; Buffer 32911.\n",
      "[...Saving model - episode 1517;T 794 ; episode_max=2.10; last_100_avg_score=0.1295 path 6_11_train_v2\n",
      "[...Saving model - episode 1522;T 0 ; episode_max=2.60; last_100_avg_score=0.1567 path 6_11_train_v2\n",
      "Solved environment after 1575 Episodes\t Last Max Agent Score: 1.30\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "from workspace_utils import active_session\n",
    "import maddpg as maddpg_package\n",
    "importlib.reload(maddpg_package)\n",
    "import Agent\n",
    "importlib.reload(Agent)\n",
    "maddpg_agent = maddpg_package.MADDPG( lr_actor=2e-4, lr_critic=1e-3, state_size=state_size, action_size=action_size, random_seed=0)\n",
    "\n",
    "with active_session():\n",
    "    final_scores_train=ddpg_ma_version(agent=maddpg_agent,start_noise=5,eval_mode=False, noise_reduction=0.9999, min_noise=0.001,  n_episodes=4000, path_to_write_suffix='6_11_train_v2')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot of rewards per episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xl8XXWd//HXJ1vbNGnTNum+hEIKtMhSKtBRBATZhZ8jCoqIjsrPFRn9DcM2uIzOuOKMgyMwgiLwQxARq1QRBFkEKmktpbSUphtNC226pWvaLJ/545x7enNzk9ykOffeJO/n43Efufec773nk5Pc8znnux1zd0RERAAKch2AiIjkDyUFERGJKCmIiEhESUFERCJKCiIiElFSEBGRiJKCiIhElBRERCSipCAiIpGiXAfQU5WVlV5dXZ3rMERE+pWFCxducfeq7sr1u6RQXV1NbW1trsMQEelXzGxdJuVUfSQiIhElBRERiSgpiIhIRElBREQiSgoiIhJRUhARkYiSgoiIRJQURERypK3NebB2Pc2tbdGy1Q27+dffLeP5ui0APLdyC799eSNfnfcqTc2tscfU7waviYgMFI8s3sC1Dy1hU2MTXzizBoAvPfgyi9fv4M7n1rD2WxfwkTsXROXb3Pn6xcfEGpOuFEREcmTH3mYAtu45EC177a2dnZbfvHN/7DEpKYiI5IhZ8NPdMyrvZFbuUCgpiIjkSJgTsnCoz1xsScHMppjZU2a23MxeNbMvpilzupk1mtni8HFzXPGIiEj34mxobgG+7O6LzKwcWGhmj7v7spRyz7r7hTHGISIiGYrtSsHd33T3ReHzXcByYFJc2xMR6W8sbFTIsEkh43KHIittCmZWDZwALEizeq6ZvWxmvzezWdmIR0QkHyQamvNJ7OMUzKwM+BVwjbun9rVaBExz991mdj7wCFCT5jOuAq4CmDp1aswRi4hkVzZ6FWUq1isFMysmSAj3ufvDqevdfae77w6fzweKzawyTbk73H2Ou8+pqur2bnIiIv1CHl4oxNr7yIA7geXufksnZcaH5TCzk8J4tsYVk4hIPsq4TSHeMIB4q4/eAVwBvGJmi8NlNwBTAdz9NuAS4DNm1gLsAy7zTEdxiIj0d3nYqBBbUnD35+jm6sjdbwVujSsGEZH+IJ/OhDWiWUQkR6IRzXmUFZQURERypKe1RwNmnIKIiPQPSgoiIjmXP/VHSgoiIjli9Gyai2xQUhARyZGD91PI9B26n4KIiGSRkoKIiESUFEREcuTgndfyp1FBSUFEJEe6a1P43mMr2r3WOAURkQHMupkn9dan6rIUyUFKCiIiElFSEBHJsfxpUVBSEBHJnfybOVtJQUQk1zSiWUREetwlNRu5Q0lBRCRHLA/vvKakICIiESUFEZFcU5uCiIgcbFPIjGehRVpJQUQkRw5Oc5E/lwpKCiIiElFSEBGRiJKCiEiORNVHGZbXOAURkQHK3fnN4o3h8xwHk0RJQUQkB55Yvpk/r2jIdRgdKCmIiORA477m6HkeXSgoKYiI9Be685qIyCAwKMYpmNkUM3vKzJab2atm9sU0ZczMfmhmdWa2xMxmxxWPiIh0ryjGz24Bvuzui8ysHFhoZo+7+7KkMucBNeHjZODH4U8RkUEjf64TYrxScPc33X1R+HwXsByYlFLsYuDnHngRqDCzCXHFJCKSlzLMCgNmnIKZVQMnAAtSVk0C1ie9rqdj4hARkSyJPSmYWRnwK+Aad9+ZujrNWzokQzO7ysxqzay2oSH/+vWKiAwUsSYFMysmSAj3ufvDaYrUA1OSXk8GNqYWcvc73H2Ou8+pqqqKJ1gRkSxK7nGU6e04syHO3kcG3Aksd/dbOik2D/ho2AvpFKDR3d+MKyYRkXyUaY/UbHRdjbP30TuAK4BXzGxxuOwGYCqAu98GzAfOB+qAvcDHY4xHRCRv5OP9mSHGpODuz5G+zSC5jAOfiysGEZF8lU8D1pJpRLOISI7lU35QUhARyYHk6qOVm3flMJL2lBRERHIgufpoVcMefv9KfvSxUVIQEckDKzblx9WCkoKISA6k9j7Kl3YFJQURkRzoTe8j3U9BRGSQyJdhC0oKIiI5oOojERGJ9Kr6KAtzJCkpiIjkiVfqG2lqbstpDEoKIiJ54r23PpfrEJQURETkICUFEZF+Ql1SRUQkq5QUREQkoqQgIiIRJQURkX5CbQoiIpJVSgoiInkgT2a5UFIQEZGDlBRERPoJzX0kIiJZpaQgIiIRJQUREYkoKYiI9BMapyAiIlmlpCAiIhElBRERiSgpiIj0E9kY9RxbUjCzu8xss5kt7WT96WbWaGaLw8fNccUiIiKZKYrxs38G3Ar8vIsyz7r7hTHGICIiPRDblYK7PwNsi+vzRUSk7+W6TWGumb1sZr83s1k5jkVEJHeyMQghA3FWH3VnETDN3Xeb2fnAI0BNuoJmdhVwFcDUqVOzF6GIyCCTsysFd9/p7rvD5/OBYjOr7KTsHe4+x93nVFVVZTVOEZG8MZBHNJvZeDOz8PlJYSxbcxWPiIj0oPrIzN4J1Lj7T82sCihz9zVdlL8fOB2oNLN64CtAMYC73wZcAnzGzFqAfcBl7nlSqSYiMkhllBTM7CvAHOBI4KcEB/d7gXd09h53/1BXn+nutxJ0WRURkTyRafXR+4CLgD0A7r4RKI8rKBER6Sif7rx2IKzacQAzGx5fSCIi/cu6rXtoaW3LdRh9ItOk8KCZ3Q5UmNmngCeA/4kvLBGR/qF++15O++6f+e4fV+Q6lD6RUZuCu3/PzN4D7CRoV7jZ3R+PNTIRkX6gYdd+AF5cPTAmcOg2KZhZIfCYu58FKBGIiORIXtx5zd1bgb1mNjL+cEREBqd86Y+f6TiFJuAVM3ucsAcSgLtfHUtUIiKSE5kmhUfDh4iIxMByHUAo04bmu82sBJgRLlrh7s3xhSUiMrhkUn2UjSqmTEc0nw7cDawlSGhTzOzK8J4JIiIyQGRaffR94Gx3XwFgZjOA+4ET4wpMRGQg681Zf2VZSZ/HkSrTwWvFiYQA4O6vE05uJyIi2fHJU6fHvo1MrxRqzexO4J7w9eXAwnhCEhHph3o4iCBfGpZTZZoUPgN8Dria4Hd5BvjvuIISERno8mVcQqpMk0IR8J/ufgtEo5yHxBaViIh0kI2ri0zbFP4EDEt6PYxgUjwREemFfK0+yjQpDE3cTxkgfF4aT0giIgNfvlYfZZoU9pjZ7MQLM5tDcAtNEREZQDJtU7gG+KWZbSRIcBOBS2OLSkRkgOtN9ZFloc6pyysFM3u7mY1395eAo4AHgBbgD8Ca+MMTERmYUquPsjEtdia6qz66HTgQPp8L3AD8CNgO3BFjXCIikgPdVR8VunvidkKXAne4+6+AX5nZ4nhDExEZuPpr76NCM0skjjOBJ5PWZdoeISIiKXpXWxR/KunuwH4/8LSZbSHobfQsgJkdATTGHJuIiGRZl0nB3b9pZn8CJgB/dI+aQgqAL8QdnIiIZFe3VUDu/mKaZa/HE46IiORSpoPXREQkx3I+TkFERLpm2ThSZ5GSgoiIRGJLCmZ2l5ltNrOlnaw3M/uhmdWZ2ZLkuZVERCQ34rxS+BlwbhfrzwNqwsdVwI9jjEVEpN/Lp/sp9Ji7PwNs66LIxcDPPfAiUGFmE+KKR0REupfLNoVJwPqk1/XhMhERyZFcJoV0V0JpR36b2VVmVmtmtQ0NDTGHJSKSfZ7BxBfZ6OmUy6RQD0xJej0Z2JiuoLvf4e5z3H1OVVVVVoITERmMcpkU5gEfDXshnQI0uvubOYxHRGTQi22mUzO7HzgdqDSzeuArQDGAu98GzAfOB+qAvcDH44pFREQyE1tScPcPdbPegc/FtX0RkYGmX3dJFRGR/kdJQUREIkoKIiJ54IVVW3MdAqCkICKSFxa9saPbMpo6W0REskpJQUREIkoKIiJ9oPtJKvoHJQURkX7CsjBSQUlBREQisY1oFhGRjh6sXc/hVWW5DqNTulIQEcmiax9awvt//Hyuw+iUkoKISD+hcQoiIpJVSgoiIhJRUhARkYiSgoiIRJQUREQkoqQgIiIRJQURkX5CXVJFRCSrlBRERCSipCAicgjcezlpdp7Ota2kICLST2jqbBGRgSoLjca9oaQgInIIel0LpOojERHJd0oKIiKHoLftzL2pPtI4BRGRgUrVRyIiA1GeHt17KdakYGbnmtkKM6szs+vSrP+YmTWY2eLw8ck44xERyRt52vuoKK4PNrNC4EfAe4B64CUzm+fuy1KKPuDun48rDhGROPW6TaEX7+vvbQonAXXuvtrdDwC/AC6OcXsiInKI4kwKk4D1Sa/rw2Wp3m9mS8zsITObEmM8IiJ9rtctCnlafRRnUkj3K6fuv98C1e5+LPAEcHfaDzK7ysxqzay2oaGhj8MUEekb1dc9ylfnvZpZ4d5UH/XzaS7qgeQz/8nAxuQC7r7V3feHL/8HODHdB7n7He4+x93nVFVVxRKsiEhvpLYp/Oz5tTmJo6/EmRReAmrM7DAzKwEuA+YlFzCzCUkvLwKWxxiPiIh0I7beR+7eYmafBx4DCoG73P1VM/s6UOvu84CrzewioAXYBnwsrnhEROLQ66mz81RsSQHA3ecD81OW3Zz0/Hrg+jhjEBHJhrYsJIf+3iVVRGTAS6SCgXLBoKQgItIH2nqYFDbs2BdPIIdISUFEpA901rawdEMjr25s7FDmP/+0Mitx9VSsbQoiIgNd4jjfWZvChf/1HABrv3XBIVcxZWO8m64URET6QCYH/P7Q7KCkICJyCDw81GfS+6g/dF9VUhAR6QO6UhARkehIn8kB/5DbFDROQUSkf8io+qgfXCsoKYiIHILEYT45KdzxzCrWb9vbrlz99r3c/vTqLEbWO0oKIiJ9IPlC4d/mv8ZH7/pru/Wf+Fkttzz+epaj6jklBRGRQ5BIBqm1R7v3t3T5unf69/0UREQGjWxMiJcNSgoiIocg0Xg8QHKCkoKISF/Q1NkiIpI091Fu4+grSgoiIn2ifVbor9VJSgrdWNWwu1/MVyLt3fTIK1Rf92iuw+iRI26Yzz8/tKTLMve8uI7q6x5l74G+6Mly0Nk/eJr3//j5DstbWttYu2VPn25roNm0swmA1pRLhS2797OrqTkXIR0SJYUuLFy3jTO//zT3vLgu16FID9374hu5DqHHWtqcB2rXd1nm9qdXAbB194E+3fbrm3azcN32Dsu/+9gKTv/en6nfvjfNuwTgn8JEnu7U8fwfPtun29LU2Tm2ZkvwRVi8fkeOIxHJjRdWbwX6PgkNRG1pGhXWb8vPu6t1RUlBRKQPDJRaZiWFTAyQP7YMHNk+AOkr0L2Bso+UFERE+kB2xilomov8kI3WHYnFQO05NlCmVOjvknscDZQ/SVGuA8i2x5dt4rQZVZQUFbB2yx52NjWzeed+zpo5LiqzeP0OxpYP4cnXNrV775ote2hqbuXoCSPaLd+6ez91m3dz8vQxvLBqK0eOL2f08JJuY3F3/rhsE7OnjuI3izewv6WN98+ezPiRQwF4pb6R3y7ZyDmzxnHitNFs2LGPlZt2UVRQwDtrKqPPeXH1Vg60tPH6pl184p2HRWcTu/e3cOuTdXzmtMMZWVoMwN4DLSxYs43Taqp4fPkmzp45jrrNuzGD/S1tDC8pwoF9B1qZOfHg7zn/lTf5S90Wzj1mPCs37WbSqGGcM2s8rW3Odx9bwRVzpzGpYhhPvraJXU0tXHz8JP72xnbGjxzKzn0tPLpkIydMHcUZR40F4MnXNrFtTzMjhxUzYeRQtu89wJjhQ5g5cUT0N3qrsYmdTc0cM2kkAHv2t/DS2m2cMGUUz9VtYUxZCadMH8PGHfvYvGs/1WNKue3p1Xzh3UdEcf+ytp7F9TuYNXEE7zthEks37KRu824+MGcyz63cwtINjXzw7VO44eFXOLF6FGfPHMcRY8sBaGpu5flVW3j3UeNo3NvMoje2s2X3fs49ZjzlQ4tpaW3jN4s3cvzUChas3sac6lFs23OAR5e8yQlTK9i+t5nLT57KXX9Zw5RRpcyaOILpVWUAPPjSejY27uPyk6excN22qFMDwK1PruS4KRU07gv2T9mQIu598Q1KSwqp3x40XF7zwGIuOm4iBQa/X/oW1557JK+9tYuJFcPYuGMfE0cOY+XmXazavIemllZmTx1FcWEBO/YdYMzwEhp27Wfhuu1MHV1KzbjyaNvXPvQyM8aVM71qODv2NrOkvhGAbz66jGvOmsEjf9vAzqZmCguMo8ePYGNjE2bwVmMTU0eXMmJYMa+9uZM51aN4eNEG/u9p06kZW86v/7aBdVv3cOT4crbsOsDQ4gJ27GumsmwIu5qaOXZyBSOGFfPCqq3samqmpKiAogKjsKCA8982npLCAmrXbae5tY0Lj53Anv2tPP16Ay2tbYwbOZSm5jaamlvZ1dTC9Krh0bq3V4+mfvs+1m/by0mHjebDJ0+lfGhx2u9j4v/o+CkV3X53IWhY/q8nV0av9zW3dlm+ubUto8/NNetvZ1Jz5szx2traXr33Xx5Zyj0vruOo8eX84Zp3tevHfv+nTmHu4WMAOvRv//vZk7jlg8dHy9d+64J268/8/p9Z1bCHmy44mm88upzDKodz5lFj+fLZRzKspJCfPLuauYePYdbEke3e94elb/Hpexe2W1ZaUsiyr5/bIY6137qA4772Rxr3Bf2ea286i8qyIbS1OdNvmB+V+/b738alb58KwOU/eZG/1G1tF/OXHlzMw4s2cOXcadz9wjq+/4Hj+PIvX067vxLv2b7nACf86+Md1j969Tt5vm4r35y/HIDV/3Z+FMuz157Bqd95ipLCAg4kfRnu/cTJjBhWxEW3/iXtNmdNHMGrG3fy2dMP57//vCqKY/H6HfyfHwXvqSofQsOu/QCs+ffzqbnx97S0OZVlQ9iyez9nHT2OJ5ZvSvv5Cd983zHc+OulHZZPHDmU568/EwjGOnTWtfUfz5rBD59c2aFvene+8t6ZfO23y3r0HulbE0cO5Yhx5Rw1vpw7nul4f4NRpcVs39v5+ILTZlTx9OsNcYbYqcU3v4eK0u5PONMxs4XuPqe7coPqSiEx3uC1t3Z1WLdjbxdd7rr53q9qCAb3fOPR4OC4ZssefvLcGsaOGMJV7zo8Wp6aTBp2NXX4rL0HOj/bSCQEgAMtwYG2ua392cemnfuj569u3Nkx1s27AVixKdgHm9LEkKqpJX1Mu5pa2LDjYJe75FgSZ00HUs6OPnLnAn7+Dyd1uq1EzG+k3KAkkRCAKCEkPr8lPDBv2R0sX/5mx9871b5O9vPGxoP7Y00Xg7Z+8ETv5sVXQsi9jY1NbGxs4plODuxdJQQg1oTw7LVnMHbEEF5/azfvvfU5IDjx2dfcSlFBASVF8df4D6qkkG3NrU5LTJeMiTyVSA7ptLb2zVVgpheTXcUSl+Y0v2Mml+mFBWookuy7cu40du9v5VeL6jusSz5pHFJ88OBvZpSWZO9QrYbmmO2P6UCZSDbpDooJqWfp6VgGreiZ1oUmx3LoNyjP7KCdLhFlEq9SguSz4sLcHZpj3bKZnWtmK8yszsyuS7N+iJk9EK5fYGbVccaTib5sY3H32M6eE5/b1ed3dXDsya/Z9ecc/KDkWPZ3UuXU19LFlosrFpG+lI1qos7EtmUzKwR+BJwHzAQ+ZGYzU4p9Atju7kcAPwC+HVc8mUp75n0Ip5WZnK2n011yOhBdKXT++V21gfakJ0RnVzttbc6BpP2V/Jl79mcnKaS/Uug+47UMlHmOZUAqLszdtWyc6egkoM7dV7v7AeAXwMUpZS4G7g6fPwScadkYndGFtAfLXh4/zKzXZ63d9WpJHPhSk05nOy91XpZMDpzdlW1u83ZXBMmxdDWLZ0tbz/ZJVwkyXdLNJBFnK2mJ9MaQwsKcbTu2Lqlmdglwrrt/Mnx9BXCyu38+qczSsEx9+HpVWGZLZ5/b2y6pT7/ewJV3/TV6XTO2jJVhTxyA8SOGUj60iFZ3Vjd07HWSXL5mbFm7dcmfk2xocQHjRgxl3da9ad+3Zff+tD0dasaW0eYe9WpKF+/kUcMYVlzI/pa2dj11hhQVMHV0aYe4Dq8aToFZh1hLSwo77fGUiHdfc2vUPz7ZxJFD2/XWmTq6NIpl/IihvLUzfc+mCSOH8mZj972eEo4YW0ZdJ/s4eZs90VW3w8Tv3dnfVaS3PnXqYexrbk3b1Tm5oXnvgRZm3vwYJUUFvP6N8/pk25l2SY3zSiHdSWtqBsqkDGZ2lZnVmlltQ0PvuoOVDSliUsUwAKrHlFIzrozplcOpCAd1zZ5WQc24Mo4aX87wkkJmJg1QO2fWOGrGlVE9ppSq8iHUjCtr9zipejQARWGPlpMOC16fceRYZk0cgRnMmTaqw/sS4yLOmXVw4Nyxk0dSM66MI8eXM2Jo0OOgorSYmnFlnD2zY7ljJrUfSPfuo8ZGn39mOFCstKSQI8eXUzOujHeHy847ZjwQ9LmeVDGMSRXDGDO8hOlVw5leOZzKsoO/57GTD46vSL6OO35qRfQ5J04b1S6W2dMqKC0p5JhJIxg/Ymi0/LQZVZwwtf3goMTfIPET4OyZ46gZW0ZFaTEzxpVxXNKAor8L9xvAMZNGMGviCEpLCnnXjCoAzjp6LMdPqWB65XCGFbc/40p0Opp7+BhGhdtL/L0ATpk+Ovq9zzp6LKlSL+uHFhe0e39C6nYTJowcmnZ5QkkXDYyp9cydbePUpIGNCVXlQ5heNbzD8uMmtx87M7yksNPPPWp8edrlRyYNfkv+X87EtDHBCcwRKSdM6VSWlUT/F6mGFRdy7OSRTK8c3mmcp9ZUtvvdOuuBdvqRVe1el5YUtvuZ/BnTK4dz+xUnMmNcGRccO4EffXh2tP6o8eV8cM5kSgoLeOjTc7nqXdO5+swa/umcozi1ppKbLjiamy44ms+dcTj3ffLklG0W8dnTD+eOK07sbrf0uTivFOYCX3X3c8LX1wO4+78nlXksLPOCmRUBbwFV3kVQhzJ4TURksMqHK4WXgBozO8zMSoDLgHkpZeYBV4bPLwGe7CohiIhIvGIbEeHuLWb2eeAxoBC4y91fNbOvA7XuPg+4E7jHzOqAbQSJQ0REciTWYXLuPh+Yn7Ls5qTnTcAH4oxBREQypxHNIiISUVIQEZGIkoKIiESUFEREJKKkICIikX535zUzawDW9fLtlUCnU2jkmGLruXyNCxRbb+RrXDAwYpvm7lXdFep3SeFQmFltJiP6ckGx9Vy+xgWKrTfyNS4YXLGp+khERCJKCiIiEhlsSeGOXAfQBcXWc/kaFyi23sjXuGAQxTao2hRERKRrg+1KQUREujBokoKZnWtmK8yszsyuy/K2p5jZU2a23MxeNbMvhstHm9njZrYy/DkqXG5m9sMw1iVmNjsLMRaa2d/M7Hfh68PMbEEY2wPh9OeY2ZDwdV24vjrmuCrM7CEzey3cf3PzYb+Z2T+Gf8ulZna/mQ3N1T4zs7vMbHN4J8PEsh7vIzO7Miy/0syuTLetPortu+Hfc4mZ/drMKpLWXR/GtsLMzkla3uff33SxJa37f2bmZlYZvs7afussLjP7QrgPXjWz7yQt79t95u4D/kEwdfcqYDpQArwMzMzi9icAs8Pn5cDrwEzgO8B14fLrgG+Hz88Hfk9wZ7pTgAVZiPFLwP8Hfhe+fhC4LHx+G/CZ8PlngdvC55cBD8Qc193AJ8PnJUBFrvcbMAlYAwxL2lcfy9U+A94FzAaWJi3r0T4CRgOrw5+jwuejYortbKAofP7tpNhmht/NIcBh4Xe2MK7vb7rYwuVTCKb8XwdUZnu/dbLPzgCeAIaEr8fGtc9i+zLn0wOYCzyW9Pp64PocxvMb4D3ACmBCuGwCsCJ8fjvwoaTyUbmY4pkM/Al4N/C78B9/S9IXN9p/4Zdlbvi8KCxnMcU1guDgaynLc7rfCJLC+vBAUBTus3Nyuc+A6pSDSI/2EfAh4Pak5e3K9WVsKeveB9wXPm/3vUzstzi/v+liAx4CjgPWcjApZHW/pfl7PgiclaZcn++zwVJ9lPgSJ9SHy7IurDo4AVgAjHP3NwHCn4mbAmc73v8ArgXawtdjgB3u3pJm+1Fs4frGsHwcpgMNwE/Dqq2fmNlwcrzf3H0D8D3gDeBNgn2wkPzYZwk93Ue5+o78A8EZeF7EZmYXARvc/eWUVbmObQZwalj9+LSZvT2uuAZLUkh3h+6sd7syszLgV8A17r6zq6JplsUSr5ldCGx294UZbj+b+7KI4DL6x+5+ArCHoCqkM1mJLayfv5jgcn0iMBw4r4tt58X/X6izWLIeo5ndCLQA9yUWdRJDtv6upcCNwM3pVncSQ7b2WxFB9dQpwD8BD5qZxRHXYEkK9QT1hAmTgY3ZDMDMigkSwn3u/nC4eJOZTQjXTwA2h8uzGe87gIvMbC3wC4IqpP8AKswscWe+5O1HsYXrRxLcSjUO9UC9uy8IXz9EkCRyvd/OAta4e4O7NwMPA39HfuyzhJ7uo6x+R8IG2QuByz2s38iD2A4nSPQvh9+HycAiMxufB7HVAw974K8EV/WVccQ1WJLCS0BN2DukhKCxb162Nh5m9DuB5e5+S9KqeUCit8KVBG0NieUfDXs8nAI0JqoC+pq7X+/uk929mmC/POnulwNPAZd0Elsi5kvC8rGcUbr7W8B6MzsyXHQmsIzc77c3gFPMrDT82ybiyvk+S9LTffQYcLaZjQqvhM4Ol/U5MzsX+GfgInffmxLzZRb01joMqAH+Spa+v+7+iruPdffq8PtQT9BB5C1yv98eIThhw8xmEDQebyGOfdYXjTX94UHQe+B1ghb5G7O87XcSXLotARaHj/MJ6pX/BKwMf44OyxvwozDWV4A5WYrzdA72Ppoe/nPVAb/kYK+HoeHrunD99JhjOh6oDffdIwSX0Dnfb8DXgNeApcA9BL0/crLPgPsJ2jaaCQ5kn+jNPiKo368LHx+PMbY6gvruxHfhtqTyN4axrQDOS1re59/fdLGlrF/LwYbmrO23TvZZCXBv+P+2CHh3XPtMI5pFRCQyWKqPREQkA0oKIiISUVIQEZGIkoIUqT/6AAACfklEQVSIiESUFEREJKKkIIOGmbWa2eKkR5czR5rZp83so32w3bWJ2TZ7+L5zzOyrYR/4+Ycah0gmirovIjJg7HP34zMt7O63xRlMBk4lGBD3LuAvOY5FBgklBRn0wikNHiCYnhjgw+5eZ2ZfBXa7+/fM7Grg0wRz9Sxz98vMbDRwF8Ggtb3AVe6+xMzGEAxAqiIYrGZJ2/oIcDXBYKQFwGfdvTUlnksJZrWcTjDH0jhgp5md7O4XxbEPRBJUfSSDybCU6qNLk9btdPeTgFsJ5n5KdR1wgrsfS5AcIBjV/Ldw2Q3Az8PlXwGe82ASv3nAVAAzOxq4FHhHeMXSClyeuiF3f4CD8+m/jWAU6wlKCJINulKQwaSr6qP7k37+IM36JcB9ZvYIwXQbEExf8n4Ad3/SzMaY2UiC6p6/D5c/ambbw/JnAicCLwVTJjGMgxPVpaohmJ4AoNTdd2Xw+4kcMiUFkYB38jzhAoKD/UXAv5jZLLqenjjdZxhwt7tf31UgZlZLMANmkZktAyaY2WLgC+7+bNe/hsihUfWRSODSpJ8vJK8wswJgirs/RXAzogqgDHiGsPrHzE4Htnhwn4zk5ecRTOIHwcR0l5jZ2HDdaDOblhqIu88BHiVoT/gOwWRmxyshSDboSkEGk2HhGXfCH9w90S11iJktIDhR+lDK+wqBe8OqIQN+4O47wobon5rZEoKG5sRU1V8D7jezRcDTBFNt4+7LzOwm4I9homkGPkdwL+BUswkapD8L3JJmvUgsNEuqDHph76M57r4l17GI5Jqqj0REJKIrBRERiehKQUREIkoKIiISUVIQEZGIkoKIiESUFEREJKKkICIikf8F+Y82mm77Kw0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbfb2b43f60>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "# ax = plt.axes()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(final_scores_train)+1), final_scores_train)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
